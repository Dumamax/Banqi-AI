{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Intelligence for Banqi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By:**\n",
    "- Max Maier: maierwm@rams.colostate.edu\n",
    "- Nicholas Walter: nickwalt@rams.colostate.edu\n",
    "- Tyler Hogenson:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this report, we will go over not only the game Banqi, but the different algorithms we test in our development of an AI opponent. We will first go over a quick review of what Banqi is and how it is played."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is Banqi and how is it played?\n",
    "Banqi is a version of Chinese Chess, and is often called Half Chess, Blind Chess, or Dark Chess. It takes place on a 4x8 grid, and the game begin with 32 tokens face down randomly arranged on the board (16 red pieces and 16 black). During your turn, you can either flip a piece face up, move a token, or attack an opponent's piece. A token can only move one space either horizontally or vertically. Attacks are made the same way a move is made (excluding a cannon, which we will cover later). These pieces have different ranks that determine what they can do:\n",
    "- Rank 7: General\n",
    "- Rank 6: Advisor\n",
    "- Rank 5: Elephant\n",
    "- Rank 4: Chariot\n",
    "- Rank 3: Horse\n",
    "- Rank 2: Cannon\n",
    "- Rank 1: Soldier\n",
    "\n",
    "A higher ranked piece can take a piece of equal or lower value in all but 3 cases. A Soldier can take a General, a General cannot take a Soldier, and a cannon can take any ranked piece as long as it correctly attacks. For a cannon, attacking is made by moving any distance and jumping over exactly one piece. It has to jump over exactly one piece somewhere between it's starting position and ending position.\n",
    "\n",
    "Turns continue back and forth until one player takes all of their enemies token, at which point they are considered the winner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this section we talk about what algorithms we used, how they work, helper methods, and any other information we gathered along the way.*\n",
    "\n",
    "In our project proposal, we talked about implementing 3 different algorithms: Negamax, Reinforcement Learning, and aStar Search. Right off the bat we decided that using aStar Search wasn't a viable option because it wasn't really meant to solve this kind of problem. aStar is usually used for graph traversal and path finding, but our problem needs to find some optimal solution for a state that most likely has not been seen before, and has unpredictable changes due to opponant moves. This reasoning caused us to abandon the aStar algorithm and focus more on Negamax and Reinforcement Learning.\n",
    "\n",
    "**Negamax with Alpha-Beta pruning:** \n",
    "Negamax was fairly easy to implement once we had our validMoves() and makeMove() functions working correctly. Other than a few bugs we encountered here and there, it was implemented mostly the same as it was in python. We did quite a bit of messing around with the depth limit as well as how scores were calculated. In the end, these two variables were the main two factors in how well the AI played from game to game. We found that if the depth limit was too low, the AI always seemed to make the greedy move, but if it was too high, it would make nonsensical moves in the hopes that the opponent would make exactly the right moves to lead it to the victory with the highest score. We also found that if certain pieces had too high or low of a score associated with them, the AI would make either too risky or too cautious of decisions. We will go into more details of all this in the results section.\n",
    "\n",
    "**Reinforcement Learning using a Q-Table:** \n",
    "Reinforcement Learning proved to be a much bigger challenge for us in many ways. Not only was it a bit more of a complicated algorithm to implement, but our problem space is *much* bigger than any one we dealt with in class. After running our AI against a Negamax opponent for many hours on end, we found our Q-Table to be millions of entries long, and yet the probability of ever using one of these entries again was nearly zero. Given enough time, it would be possible to nearly populate the whole Q-Table, but we predict it would take many months if not years of continual running to get anywhere near its goal. Once we realized this, we decided to take a slightly different approach to show that our algorithm was indeed working. We decided to run test games where the starting board state was always the same so we could show that our Q-table was populating and being used correctly. Even with this change, we are still looking at a table with over 32 factorial entries, and that only includes flips. Although this isn't applicable to the real game as a whole, it did give us a lot of valuable information (which will will cover in the results section).\n",
    "\n",
    "**Helper Methods:** \n",
    "We have many different helper methods throughout our code to help the algorithms run correctly. Although many of these methods are just extracted code to make things simpler and less messy, there are a few methods which are directly tied to (and a fundamental part of) the algorithms. We will breifly discuss these methods below:\n",
    "\n",
    "*validMoves():* This method is passed the state of the board, and returns a list of all possible moves that can be made (moves, flips, and attacks). The state given to the method is passed as a String where each token is represented by 3 characters. For example, 'B7U' would be a **B**lack token, rank **7** (General), and is Face-**U**p. 'R2D' is a **R**ed token, rank **2** (Cannon), and is Face-**D**own. 'XXX' indicates that the tile in that position does not have a token on it. The string consists of 32 of these character triplets where the index of the token correlates with a specific spot of the board. validMoves() proved to take more work than we expected since there are a lot of things to take into account when determining all possible moves for a given state.\n",
    "\n",
    "*makeMove():* This method is passed the state of the board and the move that is going to be made, and returns the new state of the board after the move. The move is given as an int array of 4 numbers, with the first two being the x and y position of the piece you want to move, and the last two numbers are the x and y or where you want to move that piece. This methods assumes that this move is valid, and will attempt to make the move regardless. This is alright as long as error checking is done before the method is called to ensure a correct move is being made. We resolve this issue by only ever calling this method with a move returned from validMoves(), on the board state that is given to validMoves(). This ended up being a fairly easy method to implement with not too many issues or bugs.\n",
    "\n",
    "*calculateScore():* This methods is passed the state of the board and the color of the player the score is going to be calculated for. We went through a ferw iterations of this method before we settled on something that made sense and seemed to work for us. How we decided to calculate the score was to read the entire board and add up the ranks of the pieces for each player. As we step through the board, we read each token and decide what to do with the overall score. If the token is face-down, we ignore it (since we don't know who's piece it is or its rank). If the token is our piece, we add its rank to the overall score, and if it's the opponent's piece, we subtract it from the overall score. We had to change the value of some of the pieces to something other than just their rank since a few pieces are more valuable than they first appear. For insteance, the cannon can take any ranking piece with its attack, making its value higher than 2. We also take into account the fact that soldiers (rank 1) can take generals (rank 7) by checking to see if we have more soldiers on the board than they do generals. There are a few other ways we balance the score counting overall, but we found this to be fairly balanced with the testing that we have done. Something interesting about this method of calculating scores is that they will be inverse of each other for each player. For example, if my score is 35, then my opponent's score will be -35. This didn't seem like it made much sense when we first discovered it, but we came to realize that for any given game, then better one person is doing the worse the other will be doing. If we were to keep developing this AI, we may have to tweak the score calculation slightly, but it seems to work well for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "....."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has taught us a lot about not only the game Banqi, but more importantly, about artificial intelligence algorithms. We started this project believing we already knew how things were going to turn out, and thinking it was going to be fairly easy to implement everything we needed to. As we coded, tested, and learned more about it all, we discovered that what we expected at the beginning wasn't quite as correct as we had hoped for it to be. We quickly learned that not every algorithm is applicable to every problem. We originally thought that aStar would bea good way to make an AI opponent for Banqi, and yet it doesn't seem to have much relevance at all. We also learned that an algorithm may work well for a small problem (like tic-tac-toe or Towers of Hanoi), but may simply be infeasable for a larger problem (like Chess or Banqi). We believed that Reinforcement Learning would be the most intelligent of our options, but we learned that the problem set is too large to train accurately. As our Q-table grew larger and larger (into the multiple millions of entries after hours of running), we found that equivilant board states were nearly impossible to encounter again, and we had to rethink our development strategy a bit. Although we did implement reinforcemnt learning correctly, it proved to be less helpful of an algorithm than we thought. The one thing we did guess correctly on was the implementation of Negamax. We predicted that it would be fairly quick and fairly smart, but would never be able to develop stratagies as complex as a human opponent. This prediction ended up being pretty accurate. The deeper we set the depth limit, the smarter moves the AI made, but the slower it made them. With a careful balance, we were able to have a Negamax opponent that was smart enough to give a bit of a challenge. If we had more time to develop this AI, we would have loved to try melding multiple algorithms together to created the best of both worlds. We think that Negamax is a good foundation for a Banqi opponent, but it never remembers any moves or games it plays, so it never learns or develops strategies. Given time, we would build off our Negamax foundation to possibly include another algorithm. We were looking into Neural Networks, Genetic Algorithms, as well as others that are commonly used in games like chess. This would allow the AI to not only make predictive moves based off of scores (Negamax), but it could learn from its victories and defeats to become a better player the more it plays. Overall, this was a great learning experience for usand taught us things we didn't expect to learn when starting the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Max Maier: Jupyter Notebook, validMoves(), Negamax Algorithm\n",
    "- Nicholas Walter: makeMove(), calculateScore(), other helper methods, help with algorithms and notebook\n",
    "- Tyler Hogenson: Reinforcement Learning Algorithm, User Interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Banqi Rules and Strategy: https://en.wikipedia.org/wiki/Banqi\n",
    "- Banqi game logic and UI written and provided by Ultralite Coding (our team from CS414)\n",
    "- Algorithm Help provided by Chuck Anderson in CS440"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
